# -*- coding: utf-8 -*-
"""PDF_MERGER_DEV.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sOyWMdJelY4qyILUwuOooFeefSOR1Hvl

## Instructions

Three pieces of information to handle

- Billing number (bn for short)
- Diagnostic or pdf (diagnostic)
- Order or authorization (order)


Currently there are more diagnostics than orders. That means that several diagnostics may be attached to the same order

The billing number amount is the same as the one of diagnostics.

Steps:
1. Clean data
2. Find the diagnostic-order pairs
3. For each diagnostic-order pair find the corresponding billing number
4. Merge each diagnostic-order pair in a single pdf and save it as tthe billing number related to the pair.
5. Export a .csv file with the name and the billing number of the diagnostics that could not be merged with the corresponding authorization

## Installing and importing libraries
"""

!pip install PyPDF2

!pip install unidecode

! pip install tqdm
from tqdm.notebook import tqdm

!pip install openpyxl

from PyPDF2 import PdfFileMerger
import pandas as pd
import glob
import numpy as np
from unidecode import unidecode
from numpy.linalg import norm

from google.colab import drive
drive.mount('/content/drive')

"""## 1. Clean data

### 1. Import .xlsx file with names and billing numbers
"""

import openpyxl
from pathlib import Path

path = '/content/drive/My Drive/PDF_Merger/DEV/CANTON/'

xlsx_file = Path(path, 'TRAZABILIDAD CANTON.xlsx')
wb_obj = openpyxl.load_workbook(xlsx_file) 

# Read the active sheet:
sheet = wb_obj.active

names_ls = [sheet['E' + str(i)].value[:-1] for i in range(2, sheet.max_row + 1)]
billing_ls = [sheet['P' + str(i)].value for i in range(2, sheet.max_row + 1)]

trace_dic = {'names': names_ls, 'billing': billing_ls}
traceability = pd.DataFrame(trace_dic)

def get_traceability(file_name):
  xlsx_file = Path(file_name)
  wb_obj = openpyxl.load_workbook(file_name) 

  # Read the active sheet:
  sheet = wb_obj.active

  names_ls = [sheet['E' + str(i)].value[:-1] for i in range(2, sheet.max_row + 1)]
  billing_ls = [sheet['P' + str(i)].value for i in range(2, sheet.max_row + 1)]

  trace_dic = {'names': names_ls, 'billing': billing_ls}
  traceability = pd.DataFrame(trace_dic)
  return traceability


traceability

"""
### 1. Import .csv file with names and billing numbers"""

path = '/content/drive/My Drive/PDF_Merger/DEV/BASAN/'

traceability = pd.read_csv(path + 'TRAZABILIDAD VILLAO.csv', 'unicode_escape')

traceability[:5]

values = traceability[traceability.columns[0]]
names = []
bn = []

for row in values:
  content = row.split(";")
  temp_name = content[0][:-1]
  names.append(temp_name)
  bn.append(content[1])
traceability_dc = {'Name':names, 'Bill_number':bn}  
traceability_pd = pd.DataFrame(traceability_dc, columns=['Name', 'Bill_number'])

traceability_pd[:5]

"""### 2. Get the paths of the diagnostics and the orders files

Orders and PDFs folders

Get a list with the paths of every file for the diagnostics folder and the orders folder, then we sort the alphabetically
"""

diagnostic_list_paths = []
diagnostic_list_paths = sorted(glob.glob(path + 'pdfs/*.pdf' ))

order_list_paths = []
order_list_paths = sorted(glob.glob(path + 'ordenes/*.pdf' ))

"""Now lets check the amount of orders and diagnostics that we have"""

print("Number of diagnostics: " + str(len(diagnostic_list_paths)))
print("Number of orders: " + str(len(order_list_paths)))

"""In this case there are more diagnostics than orders, which indicates that a single order could be related to several diagnostics

Path sanity check
"""

diagnostic_list_paths[:5]

order_list_paths[:5]

"""### 3. Taking just the names of the diagnostics and the orders"""

from string import digits
import string

def cleanse(list_):
  cleansed_list = []
  path_l = path.lower()
  to_remove = [path_l + 'ordenes/', path_l + 'pdfs/', '_', '.pdf', '.',
               'orden y auto', 'autorizacion', 'caderas', 'rodillas',
               'rodilla', 'codo', 'codos', 'c l s', 'cervical', 'pies',
               'columna', 'pie', 'carpograma', 'test', 'farril',
               'manos', 'torax', 'auto', 'aut', 'craneo', 'rotulas','  ']
  for row in list_:
    
    temp = row

    temp = temp.lower()
    for remove_opt in to_remove: 
      temp = temp.replace(remove_opt, '')
    remove_digits = str.maketrans('', '', digits)
    temp = temp.translate(remove_digits)
    temp = unidecode(temp)

    cleansed_list.append(temp)
  return cleansed_list

cleansed_diagnostics = cleanse(diagnostic_list_paths)
cleansed_orders = cleanse(order_list_paths)

cleansed_diagnostics[:5]

cleansed_orders[:5]

"""## 2. Find the diagnostic-order pairs

### 1. n-gram cosine similarity

Steps: 
1. Get n-gram matrix
2. Get similarity vector
3. Build df with indexes, names of orders and pdfs
"""

from sklearn.feature_extraction.text import CountVectorizer
import string

def get_Ngram(n):
  '''
  Returns n only gram
  '''
  alph = list(string.ascii_lowercase)
  alph.extend(' ')
  n_gram = []
  if n == 1:
    n_gram.extend(alph)
  else: 
    for a in get_Ngram(n-1):
      for b in alph:
        n_gram.append(a+b)
  return n_gram
def get_voc_ngrams(n):
  '''
  Returns vocabulary of n-grams + n-1-grams + ... + 1-grams 
  '''
  vocab = [] 
  for i in range(n):
    j = i+1
    vocab.extend(get_Ngram(j))
  return vocab

def get_bag_grams(corpus, n):
    #Calculate a bag of n-grams representation
    ngram_voctorizer  = CountVectorizer(ngram_range=(1,n), vocabulary = get_voc_ngrams(n), analyzer='char_wb')
    Xngram = ngram_voctorizer.fit_transform(corpus)
    Feat_ngram = ngram_voctorizer.get_feature_names()
    #Convert the n-grams to numpy arrays
    #Xngram = Xngram.toarray() 
    #If we turn this into an array the computing will be way more expensive
    #self.fit_transform() returns a sparse matrix which is better in this case
    #only for some kernels the array form is needed
    return Xngram, Feat_ngram

def get_ngram_matrix(cleansed_diagnostics, n):
  return get_bag_grams(cleansed_diagnostics, n).todense()



diagnostics_matrix, _ = get_bag_grams(cleansed_diagnostics, 2)
orders_matrix, _ = get_bag_grams(cleansed_orders, 2)

diagnostics_matrix = diagnostics_matrix.todense()
orders_matrix = orders_matrix.todense()

print("Diagnostics matrix shape: " + str(diagnostics_matrix.shape))
print("Orders matrix shape: " + str(orders_matrix.shape))

"""### Removing several orders with same name"""

def cosine_similarity(a, b): 
  a = np.ravel(a)
  b = np.ravel(b)
  return np.dot(a,b)/(norm(a)*norm(b))

def search_multiple_orders(cleansed_orders, orders_matrix):
  elements_to_remove = []
  for i in range(len(cleansed_orders)):
    for j in range(i, len(cleansed_orders)):
      similarity = cosine_similarity(orders_matrix[i], orders_matrix[j])
      if similarity >= 0.90 and (i != j):
        print("similarity: " + str(similarity) + " name_1: " + str(cleansed_orders[i])
                                               + " name_2: " + str(cleansed_orders[j]))
        elements_to_remove.append(cleansed_orders[i])
        elements_to_remove.append(cleansed_orders[j])
  return elements_to_remove

orders_remove = search_multiple_orders(cleansed_orders, orders_matrix)

indexes_order_remove = []
for order in orders_remove:
  if order in cleansed_orders:
    indexes_order_remove.append(cleansed_orders.index(order))
    cleansed_orders.remove(order)

for ix in indexes_order_remove:
  order_list_paths.pop(ix)

"""Now lets check"""

len(order_list_paths)
len(cleansed_diagnostics)

orders_matrix, _ = get_bag_grams(cleansed_orders, 2)
orders_matrix = orders_matrix.todense()
orders_matrix.shape

"""### Finding pairs"""

indexes = []
simil = []
j_ult = 0
for i in range(len(cleansed_diagnostics)):
  for j in range(len(cleansed_orders)):
    similarity = cosine_similarity(orders_matrix[j], diagnostics_matrix[i])
    if similarity >= 0.91:
      indexes.append([j, i])
      simil.append(similarity)

print("Indexes length: "+ str(len(indexes)))
min_index = np.argmin(simil)
print("Minimum similarity: "+ str(simil[min_index]))

print("Order: " + str(cleansed_orders[indexes[min_index][0]]))
print("PDF: " + str(cleansed_diagnostics[indexes[min_index][1]]))

order_pd = []
pdf_pd = []
order_ix = []
pdf_ix = []

for ind in indexes:
  order_pd.append(cleansed_orders[ind[0]])
  pdf_pd.append(cleansed_diagnostics[ind[1]]) 
  order_ix.append(ind[0])
  pdf_ix.append(ind[1])

data_c = pd.DataFrame({'Order_index': order_ix, 'Order': order_pd, 'PDF_index': pdf_ix, 'PDF': pdf_pd})
#data_c = pd.DataFrame({'Order_index': fit_index(removed_indexes, order_ix), 'Order': order_pd, 'PDF_index': pdf_ix, 'PDF': pdf_pd})

def get_missing_pdfs(cleansed_pdfs, pdf_ix):
  missing_pdfs = []
  for i in range(len(cleansed_pdfs)):
    if i not in pdf_ix:
      missing_pdfs.append(cleansed_pdfs[i])
  print('Accuracy: ' + str((len(cleansed_pdfs)-len(missing_pdfs))/len(cleansed_pdfs)))
  return missing_pdfs

missing_pdfs = get_missing_pdfs(cleansed_diagnostics, pdf_ix)
missing_pdfs
data_c[:50]

"""### Posibles Errores

Casos de error:

1. Nombres muy diferentes en orden y pdf
2. Ausencia de la orden o del pdf
3. Mas de una orden para la misma persona. En algun caso redundantes (Sacar de la lista y hacer manual)
4. Errores de tipeo en la orden que hacen que el puntero j_ult salte un pdf

Â¿Como proceder? Crear una excepcion y arrojar un mensaje para union manual, por el momento.

## 3. For each pair find the ID number
"""

pdf_pd_matrix, _ = get_bag_grams(pdf_pd, 2)
pdf_pd_matrix = pdf_pd_matrix.todense()

pdf_pd_matrix.shape

names = list(traceability_pd['Name'].values)
cleansed_names = cleanse(names)

cleansed_names[:5]

cleansed_diagnostics[:5]

names_matrix, _ = get_bag_grams(cleansed_names, 2)
names_matrix = names_matrix.todense()
names_matrix.shape

indexes = []
simil = []
i = 0
j_list = []
while i < len(pdf_pd_matrix):
  for j in range(i, len(names_matrix)):
    #print('i: ' + str(i) + ' j: ' + str(j))
    similarity = cosine_similarity(pdf_pd_matrix[i], names_matrix[j])
    if similarity >= 0.9:
      #print('i: ' + str(i) + ' j: ' + str(j))
      if j not in j_list:
        indexes.append([i, j])
        simil.append(similarity)
        i += 1
        j_list.append(j)
        break

print("Indexes length: "+ str(len(indexes)))
min_index = np.argmin(simil)
print("Minimum similarity: "+ str(simil[min_index]))

print("PDF: " + str(pdf_pd[indexes[min_index][0]]))
print("Names: " + str(cleansed_names[indexes[min_index][1]]))

bill_n = traceability_pd['Bill_number'].values

np.array(indexes)[:,1]

len(indexes)

missing_ind = []
for i in range(len(cleansed_names)):
  if i not in np.array(indexes)[:,1]:
    missing_ind.append(i)

missing_ind

missing_pdfs_csv = traceability_pd.iloc[missing_ind]

missing_pdfs_csv.to_csv(path+'Missing_files.csv', index=False)

missing_pdfs_csv

billing_ls = list(bill_n[np.array(indexes)[:,1]])
data_c['Billing_number'] = billing_ls
data_c

"""## 4. Merge pdf-order in a single pdf and save it with the billing number related to the pair"""

path_save = path + 'merged/'

def merge_all_pdfs(pdf_list_paths, order_list_paths, data_c, path_save):
  pdf_index = data_c['PDF_index']
  order_index = data_c['Order_index']
  billing_ls = data_c['Billing_number']
  for i in tqdm(range(len(pdf_index))):
    merger = PdfFileMerger(strict=False)
    merger.append(pdf_list_paths[pdf_index[i]])
    merger.append(order_list_paths[order_index[i]])
    merger.write(path_save + 'FE' + str(billing_ls[i])+'.pdf')
    merger.close()

merge_all_pdfs(diagnostic_list_paths, order_list_paths, data_c, path_save)

path = '/content/drive/My Drive/PDF_Merger/7_DICIEMBRE/GILBERTO RX/'

merged_paths = []
merged_paths = sorted(glob.glob(path + 'merged/*.pdf' ))

len(merged_paths)

pdf_index = data_c['PDF_index']

len(pdf_index)

"""Three things:

- Billing number
- Order
- pdf

Currently there are more pdfs than orders. That means that several pdfs may be attached to the same order

The billing number amount is the same as the amount of pdfs.

Steps:

1. Find the pdf-order pairs
2. For each pair find the ID number
3. Merge pdf-order in a single pdf
4. Save the pdf with the billing numer related to the pair

Casos nombres ordenes

  1. Sin numero
  2. Con numero largo
"""